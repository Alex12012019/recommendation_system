{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "54da5c06-f202-4b01-ab00-646770a14e19",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-01 23:42:04] –ó–∞–≥—Ä—É–∑–∫–∞ 10% –¥–∞–Ω–Ω—ã—Ö...\n",
      "[2025-05-01 23:42:10] –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä (user, node)...\n",
      "[2025-05-01 23:42:11] –û—Ç–ª–∞–¥–∫–∞: –æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è 10% –¥–∞–Ω–Ω—ã—Ö...\n",
      "[2025-05-01 23:42:11] –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ç–æ–≤–∞—Ä–æ–≤...\n",
      "[2025-05-01 23:42:12] num_users: 81549, num_items: 201806\n",
      "[2025-05-01 23:42:12] –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π user_id: 81548, –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π item_id: 201805\n",
      "[2025-05-01 23:42:12] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
      "[2025-05-01 23:42:12] –ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ –∫—ç—à–∞...\n",
      "[2025-05-01 23:44:19] –ó–∞–≥—Ä—É–∑–∫–∞ item —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞...\n",
      "[2025-05-01 23:44:21] –ó–∞–≥—Ä—É–∂–µ–Ω–æ 217200 –∑–∞–ø–∏—Å–µ–π –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞.\n",
      "[2025-05-01 23:44:21] –û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: 0 –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
      "[2025-05-01 23:44:21] –ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à—ë–Ω.\n",
      "[2025-05-01 23:44:21] –†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ submission.csv\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "\"\"\"baseline_qwen.ipynb\n",
    "\n",
    "Automatically generated by Colab.\n",
    "\n",
    "Original file is located at\n",
    "    https://colab.research.google.com/drive/1dvfywmCg9ng6d4tl9QvsRQuUhaiV489o\n",
    "\"\"\"\n",
    "\n",
    "import os\n",
    "import time\n",
    "import json\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm  # ‚ùå –ù–µ –∏—Å–ø–æ–ª—å–∑—É–µ–º auto –¥–ª—è –∏–∑–±–µ–∂–∞–Ω–∏—è –≤–∏–¥–∂–µ—Ç–æ–≤\n",
    "\n",
    "#from google.colab import drive\n",
    "#drive.mount('/content/drive')\n",
    "\n",
    "#DATA_DIR = \"/content/drive/MyDrive/AvitoTechML25/Data\"\n",
    "#CACHE_DIR = \"/content/drive/MyDrive/AvitoTechML25/cache\"\n",
    "DATA_DIR = \"Data\"\n",
    "CACHE_DIR = \"cache2\"\n",
    "\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "PREDICTIONS_FILE = os.path.join(CACHE_DIR, \"predictions.csv\")\n",
    "ITEM_EMBEDDINGS_FILE = os.path.join(CACHE_DIR, \"item_embeddings.pkl\")\n",
    "BEST_MODEL_PATH = os.path.join(CACHE_DIR, \"best_model.pth\")\n",
    "\n",
    "# ==============================\n",
    "# üîß –û—Å–Ω–æ–≤–Ω—ã–µ –ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "# ==============================\n",
    "IS_DEBUG = True\n",
    "DEBUG_SAMPLE_PERCENT = 0.1\n",
    "PATIENCE = 3\n",
    "\n",
    "# –í–∫–ª—é—á–∞–µ–º —Å–∏–Ω—Ö—Ä–æ–Ω–Ω—É—é —Ä–∞–±–æ—Ç—É —Å GPU\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# ==============================\n",
    "# üïí –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–µ–º\n",
    "# ==============================\n",
    "import builtins\n",
    "def tprint(*args, **kwargs):\n",
    "    current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    builtins.print(f\"[{current_time}]\", *args, **kwargs)\n",
    "print = tprint\n",
    "\n",
    "# ==============================\n",
    "# üß† 1. –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "# ==============================\n",
    "import pyarrow.parquet as pq\n",
    "import os\n",
    "\n",
    "def load_data(fraction=0.1):\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ 10% –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "\n",
    "    def load_fraction(path, fraction):\n",
    "        table = pq.read_table(path)\n",
    "        num_rows = int(len(table) * fraction)\n",
    "        return table.slice(0, num_rows).to_pandas()\n",
    "\n",
    "    data = {\n",
    "        'clickstream': load_fraction(os.path.join(DATA_DIR, \"clickstream.pq\"), fraction),\n",
    "        'cat_features': load_fraction(os.path.join(DATA_DIR, \"cat_features.pq\"), fraction),\n",
    "        'events': load_fraction(os.path.join(DATA_DIR, \"events.pq\"), fraction),\n",
    "        'test_users': load_fraction(os.path.join(DATA_DIR, \"test_users.pq\"), fraction),\n",
    "    }\n",
    "\n",
    "    return data\n",
    "\n",
    "t_start = time.time()\n",
    "data = load_data()\n",
    "\n",
    "# ==============================\n",
    "# üîç 2. –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –ø–∞—Ä (user, node)\n",
    "# ==============================\n",
    "def prepare_pairs(clickstream, events):\n",
    "    print(\"–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä (user, node)...\")\n",
    "    contact_events = events[events['is_contact'] == 1]['event'].unique()\n",
    "    clickstream['is_contact'] = clickstream['event'].isin(contact_events).astype(int)\n",
    "    grouped = clickstream.groupby(['cookie', 'node'], as_index=False)['is_contact'].sum()\n",
    "    grouped['target'] = (grouped['is_contact'] > 0).astype(int)\n",
    "    return grouped\n",
    "\n",
    "grouped = prepare_pairs(data['clickstream'], data['events'])\n",
    "\n",
    "# ==============================\n",
    "# üß™ –û—Ç–ª–∞–¥–æ—á–Ω—ã–π —Ä–µ–∂–∏–º: –≤—ã–±–æ—Ä–∫–∞ –∏–∑ N%\n",
    "# ==============================\n",
    "def sample_data(grouped, fraction=DEBUG_SAMPLE_PERCENT):\n",
    "    print(f\"–û—Ç–ª–∞–¥–∫–∞: –æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è {int(fraction * 100)}% –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    users_sampled = grouped.sample(frac=fraction, random_state=42)['cookie'].unique()\n",
    "    return grouped[grouped['cookie'].isin(users_sampled)]\n",
    "\n",
    "# ==============================\n",
    "# üî¢ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "# ==============================\n",
    "def encode_user_item(grouped):\n",
    "    print(\"–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ç–æ–≤–∞—Ä–æ–≤...\")\n",
    "    le_user = LabelEncoder()\n",
    "    le_item = LabelEncoder()\n",
    "\n",
    "    # –û–±—Ä–∞–±–æ—Ç–∫–∞ –≤–æ–∑–º–æ–∂–Ω—ã—Ö NaN\n",
    "    grouped['cookie'] = grouped['cookie'].fillna('unknown')\n",
    "    grouped['node'] = grouped['node'].fillna('unknown')\n",
    "\n",
    "    grouped['user_id'] = le_user.fit_transform(grouped['cookie'])\n",
    "    grouped['item_id'] = le_item.fit_transform(grouped['node'])\n",
    "\n",
    "    num_users = grouped['user_id'].nunique()\n",
    "    num_items = grouped['item_id'].nunique()\n",
    "\n",
    "    return grouped, num_users, num_items, le_user, le_item\n",
    "\n",
    "if IS_DEBUG:\n",
    "  grouped = sample_data(grouped)\n",
    "\n",
    "grouped, num_users, num_items, le_user, le_item = encode_user_item(grouped)\n",
    "\n",
    "print(f\"num_users: {num_users}, num_items: {num_items}\")\n",
    "print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π user_id: {grouped['user_id'].max()}, –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π item_id: {grouped['item_id'].max()}\")\n",
    "\n",
    "# ==============================\n",
    "# üß™ –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val\n",
    "# ==============================\n",
    "def create_train_val_split(grouped, val_size=0.2, random_state=42):\n",
    "    print(\"–†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ train/val...\")\n",
    "    users = grouped['user_id'].unique()\n",
    "    train_users, val_users = train_test_split(users, test_size=val_size, random_state=random_state)\n",
    "    train_mask = grouped['user_id'].isin(train_users)\n",
    "    val_mask = grouped['user_id'].isin(val_users)\n",
    "    return grouped[train_mask], grouped[val_mask]\n",
    "\n",
    "# ==============================\n",
    "# üßÆ Two-Tower –º–æ–¥–µ–ª—å\n",
    "# ==============================\n",
    "class TwoTower(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users + 2, embed_dim)\n",
    "        self.item_emb = nn.Embedding(num_items + 2, embed_dim)\n",
    "\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        u = self.user_tower(self.user_emb(users))\n",
    "        i = self.item_tower(self.item_emb(items))\n",
    "        return torch.sum(u * i, dim=-1)\n",
    "\n",
    "    def get_user_vector(self, users):\n",
    "        return self.user_tower(self.user_emb(users))\n",
    "\n",
    "    def get_item_vector(self, items):\n",
    "        return self.item_tower(self.item_emb(items))\n",
    "\n",
    "# ==============================\n",
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è 3. –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\n",
    "# ==============================\n",
    "def train_model_with_validation(grouped, num_users, num_items):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "    model = TwoTower(num_users, num_items).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "\n",
    "    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º\n",
    "    if os.path.exists(BEST_MODEL_PATH):\n",
    "        print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –æ–±—É—á–µ–Ω–Ω–æ–π –º–æ–¥–µ–ª–∏ –∏–∑ –∫—ç—à–∞...\")\n",
    "        model.load_state_dict(torch.load(BEST_MODEL_PATH))\n",
    "        return model, device\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    train_data, val_data = create_train_val_split(grouped)\n",
    "    X_train = torch.tensor(train_data[['user_id', 'item_id']].values, dtype=torch.long)\n",
    "    y_train = torch.tensor(train_data['target'].values, dtype=torch.float)\n",
    "    X_val = torch.tensor(val_data[['user_id', 'item_id']].values, dtype=torch.long)\n",
    "    y_val = torch.tensor(val_data['target'].values, dtype=torch.float)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n",
    "\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    best_loss = float('inf')\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(5):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/10\") as pbar:\n",
    "            for x_batch, y_batch in pbar:\n",
    "                users = x_batch[:, 0].to(device)\n",
    "                items = x_batch[:, 1].to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                if users.max().item() >= num_users or items.max().item() >= num_items:\n",
    "                    raise ValueError(\"‚ö†Ô∏è –ù–∞–π–¥–µ–Ω—ã user/item_id –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞!\")\n",
    "\n",
    "                with autocast(device_type=device.type):\n",
    "                    logits = model(users, items)\n",
    "                    loss = criterion(logits, y_batch)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        avg_loss = total_loss / len(train_loader)\n",
    "        print(f\"Epoch {epoch+1} | Loss: {avg_loss:.4f}\")\n",
    "\n",
    "        # --- –í–∞–ª–∏–¥–∞—Ü–∏—è ---\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                users = x_batch[:, 0].to(device)\n",
    "                items = x_batch[:, 1].to(device)\n",
    "                scores = model(users, items)\n",
    "                preds = (torch.sigmoid(scores) > 0.5).float()\n",
    "                all_preds.extend(preds.cpu())\n",
    "                all_true.extend(y_batch.cpu())\n",
    "\n",
    "        val_recall = recall_score(all_true, all_preds, average='binary')\n",
    "        print(f\"Epoch {epoch+1} | Val Recall@40: {val_recall:.4f}\")\n",
    "\n",
    "        # --- Early Stopping ---\n",
    "        if avg_loss < best_loss:\n",
    "            best_loss = avg_loss\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    print(\"–ú–æ–¥–µ–ª—å –æ–±—É—á–µ–Ω–∞ –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∞.\")\n",
    "    return model, device\n",
    "\n",
    "model, device = train_model_with_validation(grouped, num_users, num_items)\n",
    "\n",
    "# ==============================\n",
    "# üëÄ build_seen_nodes ‚Äî —Å–±–æ—Ä –∏–Ω—Ñ–æ—Ä–º–∞—Ü–∏–∏ –æ –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã—Ö –Ω–æ–¥–∞—Ö\n",
    "# ==============================\n",
    "def build_seen_nodes(clickstream):\n",
    "    seen = defaultdict(set)\n",
    "    for _, row in clickstream.iterrows():\n",
    "        cookie = row['cookie']\n",
    "        node = row['node']\n",
    "        if pd.notna(cookie) and pd.notna(node):\n",
    "            seen[cookie].add(str(node))  # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —Å—Ç—Ä–æ–∫–µ –¥–ª—è —Å—Ç–∞–±–∏–ª—å–Ω–æ—Å—Ç–∏\n",
    "    return seen\n",
    "seen_dict = build_seen_nodes(data['clickstream'])\n",
    "\n",
    "# ==============================\n",
    "# üîÑ –†–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏—è —Å –∫—ç—à–∏—Ä–æ–≤–∞–Ω–∏–µ–º\n",
    "# ==============================\n",
    "def recommend_for_users_resumable(model, device, test_users, all_nodes, le_user, seen_dict, top_k=40):\n",
    "    model.eval()\n",
    "    try:\n",
    "        all_nodes = [str(node) for node in all_nodes]\n",
    "        item_ids = torch.arange(len(all_nodes), device=device)\n",
    "\n",
    "        # --- Item —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ ---\n",
    "        if os.path.exists(ITEM_EMBEDDINGS_FILE):\n",
    "            print(\"–ó–∞–≥—Ä—É–∑–∫–∞ item —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞...\")\n",
    "            with open(ITEM_EMBEDDINGS_FILE, 'rb') as f:\n",
    "                item_embeddings = pickle.load(f)\n",
    "        else:\n",
    "            print(\"–í—ã—á–∏—Å–ª–µ–Ω–∏–µ item —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤...\")\n",
    "            with torch.no_grad():\n",
    "                item_embeddings = model.get_item_vector(item_ids).cpu().numpy()\n",
    "            with open(ITEM_EMBEDDINGS_FILE, 'wb') as f:\n",
    "                pickle.dump(item_embeddings, f)\n",
    "\n",
    "        # --- –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π ---\n",
    "        valid_cookies = []\n",
    "        encoded_ids = []\n",
    "        for _, row in test_users.iterrows():\n",
    "            cookie = row['cookie']\n",
    "            if cookie in le_user.classes_:\n",
    "                try:\n",
    "                    encoded_id = int(le_user.transform([cookie])[0])\n",
    "                    encoded_ids.append(encoded_id)\n",
    "                    valid_cookies.append(str(cookie))\n",
    "                except Exception as e:\n",
    "                    print(f\"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è cookie {cookie}: {e}\")\n",
    "\n",
    "        processed_cookies = set()\n",
    "        predictions = []\n",
    "\n",
    "        # --- –í–æ–∑–æ–±–Ω–æ–≤–ª–µ–Ω–∏–µ –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π ---\n",
    "        if os.path.exists(PREDICTIONS_FILE):\n",
    "            try:\n",
    "                df_prev = pd.read_csv(PREDICTIONS_FILE)\n",
    "                predictions = df_prev.values.tolist()\n",
    "                print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_prev)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞.\")\n",
    "                processed_cookies.update(df_prev['cookie'].astype(str).unique())\n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –ø—Ä–µ–¥—ã–¥—É—â–∏—Ö –ø—Ä–µ–¥—Å–∫–∞–∑–∞–Ω–∏–π: {e}\")\n",
    "\n",
    "        remaining = [(c, e) for c, e in zip(valid_cookies, encoded_ids) if c not in processed_cookies]\n",
    "        print(f\"–û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(remaining)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\")\n",
    "\n",
    "        BATCH_SIZE = 8192\n",
    "        start = time.time()\n",
    "\n",
    "        for i in range(0, len(remaining), BATCH_SIZE):\n",
    "            batch = remaining[i:i+BATCH_SIZE]\n",
    "            cookies_batch, encoded_batch = zip(*batch)\n",
    "            user_tensor = torch.LongTensor(encoded_batch).to(device)\n",
    "\n",
    "            with torch.no_grad():\n",
    "                user_vectors = model.get_user_vector(user_tensor).cpu().numpy()\n",
    "\n",
    "            scores = user_vectors @ item_embeddings.T\n",
    "\n",
    "            for idx, cookie in enumerate(cookies_batch):\n",
    "                ranked = [\n",
    "                    (all_nodes[j], float(scores[idx, j]))\n",
    "                    for j in np.argsort(-scores[idx])\n",
    "                    if all_nodes[j] not in seen_dict.get(cookie, set())\n",
    "                ]\n",
    "                for node, score in ranked[:top_k]:\n",
    "                    # ‚úÖ –¢–µ–ø–µ—Ä—å —Å–æ—Ö—Ä–∞–Ω—è–µ–º –≤ —Ñ–æ—Ä–º–∞—Ç–µ: node, cookie, score\n",
    "                    predictions.append([str(node), str(cookie), score])\n",
    "                processed_cookies.add(cookie)\n",
    "\n",
    "            if (i // BATCH_SIZE) % 10 == 0:\n",
    "                pd.DataFrame(predictions, columns=['node', 'cookie', 'score']).to_csv(PREDICTIONS_FILE, index=False)\n",
    "                elapsed = time.time() - start\n",
    "                print(f\"[{i + len(batch):>6}/{len(valid_cookies)}] —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ... [–í—Ä–µ–º—è: {elapsed:.2f} —Å–µ–∫.]\")\n",
    "\n",
    "        print(\"–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à—ë–Ω.\")\n",
    "        return pd.DataFrame(predictions, columns=['node', 'cookie', 'score'])\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ —Ä–µ–∫–æ–º–µ–Ω–¥–∞—Ü–∏–∏: {e}\")\n",
    "        raise\n",
    "        \n",
    "submission = recommend_for_users_resumable(\n",
    "            model=model,\n",
    "            device=device,\n",
    "            test_users=data['test_users'],\n",
    "            all_nodes=data['cat_features']['node'].unique(),\n",
    "            le_user=le_user,\n",
    "            seen_dict=seen_dict,\n",
    "            top_k=40\n",
    "        )\n",
    "\n",
    "submission['node'] = submission['node'].astype(int)\n",
    "submission['cookie'] = submission['cookie'].astype(int)\n",
    "\n",
    "# ==============================\n",
    "# üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "# ==============================\n",
    "def save_submission(df, path=\"submission.csv\"):\n",
    "    try:\n",
    "        # –£–±–µ–¥–∏—Ç–µ—Å—å, —á—Ç–æ –∫–æ–ª–æ–Ω–∫–∏ –≤ –Ω—É–∂–Ω–æ–º –ø–æ—Ä—è–¥–∫–µ\n",
    "        df[['node', 'cookie', 'score']].to_csv(path, index=False)\n",
    "        print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"–û—à–∏–±–∫–∞ —Å–æ—Ö—Ä–∞–Ω–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {e}\")\n",
    "        raise\n",
    "\n",
    "save_submission(submission)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5071c7c-7c7a-4953-9399-3c8b6907e9bd",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d916735-f93c-47e5-a38a-ce1116dab029",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[2025-05-04 04:01:30] –ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞...\n",
      "[2025-05-04 04:01:31] num_users: 116846, num_items: 408278\n",
      "[2025-05-04 04:01:31] –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π user_id: 116845, –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π item_id: 408277\n",
      "[2025-05-04 04:01:31] –ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: cuda\n",
      "[2025-05-04 04:01:31] –ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –∫—ç—à–∞...\n",
      "[2025-05-04 04:01:32] –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\n"
     ]
    }
   ],
   "source": [
    "# baseline_with_faiss.py\n",
    "\n",
    "import os\n",
    "import time\n",
    "import pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pyarrow.parquet as pq\n",
    "from collections import defaultdict\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import recall_score\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from torch.amp import autocast, GradScaler\n",
    "from tqdm import tqdm\n",
    "import faiss\n",
    "\n",
    "# ==============================\n",
    "# üìÅ –ü—É—Ç—å –∫ –¥–∞–Ω–Ω—ã–º –∏ –∫—ç—à\n",
    "# ==============================\n",
    "DATA_DIR = \"DATA\"\n",
    "CACHE_DIR = \"cache3\"\n",
    "os.makedirs(CACHE_DIR, exist_ok=True)\n",
    "\n",
    "# –§–∞–π–ª—ã –∫—ç—à–∞\n",
    "ITEM_EMBEDDINGS_FILE = os.path.join(CACHE_DIR, \"item_embeddings.pkl\")\n",
    "USER_EMBEDDINGS_FILE = os.path.join(CACHE_DIR, \"user_embeddings.pkl\")\n",
    "PREDICTIONS_FILE = os.path.join(CACHE_DIR, \"predictions.csv\")\n",
    "BEST_MODEL_PATH = os.path.join(CACHE_DIR, \"best_model.pth\")\n",
    "SAMPLED_DATA_CACHE = os.path.join(CACHE_DIR, \"sampled_grouped.pkl\")\n",
    "ENCODER_CACHE = os.path.join(CACHE_DIR, \"label_encoders.pkl\")\n",
    "\n",
    "# ==============================\n",
    "# üîß –ì–∏–ø–µ—Ä–ø–∞—Ä–∞–º–µ—Ç—Ä—ã\n",
    "# ==============================\n",
    "IS_DEBUG = True\n",
    "DEBUG_SAMPLE_PERCENT = 0.1\n",
    "MAX_EPOCHS = 5\n",
    "PATIENCE = 3\n",
    "EMBED_DIM = 256\n",
    "BATCH_SIZE_INFERENCE = 8192\n",
    "BATCH_SIZE_ITEMS = 8192\n",
    "\n",
    "# ==============================\n",
    "# üïí –õ–æ–≥–∏—Ä–æ–≤–∞–Ω–∏–µ —Å –≤—Ä–µ–º–µ–Ω–µ–º\n",
    "# ==============================\n",
    "import builtins\n",
    "def tprint(*args, **kwargs):\n",
    "    current_time = time.strftime(\"%Y-%m-%d %H:%M:%S\")\n",
    "    builtins.print(f\"[{current_time}]\", *args, **kwargs)\n",
    "print = tprint\n",
    "\n",
    "# ==============================\n",
    "# üì¶ –í–∫–ª—é—á–∞–µ–º –æ—Ç–ª–∞–¥–∫—É CUDA\n",
    "# ==============================\n",
    "os.environ[\"CUDA_LAUNCH_BLOCKING\"] = \"1\"\n",
    "torch.autograd.set_detect_anomaly(True)\n",
    "\n",
    "# ==============================\n",
    "# üß† –ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "# ==============================\n",
    "def load_data():\n",
    "    print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    data = {\n",
    "        'clickstream': pq.read_table(os.path.join(DATA_DIR, \"clickstream.pq\")).to_pandas(),\n",
    "        'cat_features': pq.read_table(os.path.join(DATA_DIR, \"cat_features.pq\")).to_pandas(),\n",
    "        'events': pq.read_table(os.path.join(DATA_DIR, \"events.pq\")).to_pandas(),\n",
    "        'test_users': pq.read_table(os.path.join(DATA_DIR, \"test_users.pq\")).to_pandas()\n",
    "    }\n",
    "    return data\n",
    "\n",
    "# ==============================\n",
    "# üîç –§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä (user, node)\n",
    "# ==============================\n",
    "def prepare_pairs(clickstream, events):\n",
    "    print(\"–§–æ—Ä–º–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–∞—Ä (user, node)...\")\n",
    "    contact_events = events[events['is_contact'] == 1]['event'].unique()\n",
    "    clickstream['is_contact'] = clickstream['event'].isin(contact_events).astype(int)\n",
    "    grouped = clickstream.groupby(['cookie', 'node'], as_index=False)['is_contact'].sum()\n",
    "    grouped['target'] = (grouped['is_contact'] > 0).astype(int)\n",
    "    return grouped\n",
    "\n",
    "# ==============================\n",
    "# üß™ –û—Ç–ª–∞–¥–æ—á–Ω—ã–π —Ä–µ–∂–∏–º: –≤—ã–±–æ—Ä–∫–∞ N%\n",
    "# ==============================\n",
    "def sample_data(grouped, fraction=DEBUG_SAMPLE_PERCENT):\n",
    "    print(f\"–û—Ç–ª–∞–¥–∫–∞: –æ—Å—Ç–∞–≤–ª—è–µ—Ç—Å—è {int(fraction * 100)}% –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "    users_sampled = grouped.sample(frac=fraction, random_state=42)['cookie'].unique()\n",
    "    return grouped[grouped['cookie'].isin(users_sampled)]\n",
    "\n",
    "# ==============================\n",
    "# üî¢ –ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ç–æ–≤–∞—Ä–æ–≤\n",
    "# ==============================\n",
    "def encode_user_item(grouped):\n",
    "    print(\"–ö–æ–¥–∏—Ä–æ–≤–∞–Ω–∏–µ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π –∏ —Ç–æ–≤–∞—Ä–æ–≤...\")\n",
    "    le_user = LabelEncoder()\n",
    "    le_item = LabelEncoder()\n",
    "\n",
    "    grouped['cookie'] = grouped['cookie'].fillna('unknown')\n",
    "    grouped['node'] = grouped['node'].fillna('unknown')\n",
    "\n",
    "    grouped['user_id'] = le_user.fit_transform(grouped['cookie'])\n",
    "    grouped['item_id'] = le_item.fit_transform(grouped['node'])\n",
    "\n",
    "    num_users = le_user.classes_.shape[0]\n",
    "    num_items = le_item.classes_.shape[0]\n",
    "\n",
    "    return grouped, num_users, num_items, le_user, le_item\n",
    "\n",
    "# ==============================\n",
    "# üßÆ Two-Tower –º–æ–¥–µ–ª—å\n",
    "# ==============================\n",
    "class TwoTower(nn.Module):\n",
    "    def __init__(self, num_users, num_items, embed_dim=256):\n",
    "        super().__init__()\n",
    "        self.user_emb = nn.Embedding(num_users + 2, embed_dim)\n",
    "        self.item_emb = nn.Embedding(num_items + 2, embed_dim)\n",
    "\n",
    "        self.user_tower = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "\n",
    "        self.item_tower = nn.Sequential(\n",
    "            nn.Linear(embed_dim, 512),\n",
    "            nn.ReLU(),\n",
    "            nn.LayerNorm(512),\n",
    "            nn.Linear(512, embed_dim)\n",
    "        )\n",
    "\n",
    "    def forward(self, users, items):\n",
    "        u = self.user_tower(self.user_emb(users))\n",
    "        i = self.item_tower(self.item_emb(items))\n",
    "        return torch.sum(u * i, dim=-1)\n",
    "\n",
    "    def get_user_vector(self, users):\n",
    "        return self.user_tower(self.user_emb(users))\n",
    "\n",
    "    def get_item_vector(self, items):\n",
    "        return self.item_tower(self.item_emb(items))\n",
    "\n",
    "# ==============================\n",
    "# üèãÔ∏è‚Äç‚ôÇÔ∏è –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏ —Å –≤–∞–ª–∏–¥–∞—Ü–∏–µ–π\n",
    "# ==============================\n",
    "def train_model_with_validation(grouped, num_users, num_items):\n",
    "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "    print(f\"–ò—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è —É—Å—Ç—Ä–æ–π—Å—Ç–≤–æ: {device}\")\n",
    "\n",
    "    # –ï—Å–ª–∏ –º–æ–¥–µ–ª—å —É–∂–µ –µ—Å—Ç—å ‚Äî –∑–∞–≥—Ä—É–∂–∞–µ–º\n",
    "    if os.path.exists(BEST_MODEL_PATH):\n",
    "        print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –º–æ–¥–µ–ª–∏ –∏–∑ –∫—ç—à–∞...\")\n",
    "        try:\n",
    "            model = TwoTower(num_users, num_items, EMBED_DIM).to(device)\n",
    "            model.load_state_dict(torch.load(BEST_MODEL_PATH, map_location=device))\n",
    "            return model, device\n",
    "        except Exception as e:\n",
    "            print(f\"–û—à–∏–±–∫–∞ –∑–∞–≥—Ä—É–∑–∫–∏ –º–æ–¥–µ–ª–∏: {e}. –û–±—É—á–µ–Ω–∏–µ —Å –Ω—É–ª—è.\")\n",
    "\n",
    "    model = TwoTower(num_users, num_items, EMBED_DIM).to(device)\n",
    "    optimizer = optim.AdamW(model.parameters(), lr=3e-4, weight_decay=0.01)\n",
    "    criterion = nn.BCEWithLogitsLoss()\n",
    "    scaler = GradScaler()\n",
    "\n",
    "    # –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ –¥–∞–Ω–Ω—ã—Ö\n",
    "    train_data, val_data = grouped.copy(), grouped.copy()\n",
    "    X_train = torch.tensor(train_data[['user_id', 'item_id']].values, dtype=torch.long)\n",
    "    y_train = torch.tensor(train_data['target'].values, dtype=torch.float)\n",
    "    X_val = torch.tensor(val_data[['user_id', 'item_id']].values, dtype=torch.long)\n",
    "    y_val = torch.tensor(val_data['target'].values, dtype=torch.float)\n",
    "\n",
    "    train_dataset = TensorDataset(X_train, y_train)\n",
    "    val_dataset = TensorDataset(X_val, y_val)\n",
    "\n",
    "    train_loader = DataLoader(train_dataset, batch_size=2048, shuffle=True)\n",
    "    val_loader = DataLoader(val_dataset, batch_size=2048, shuffle=False)\n",
    "\n",
    "    best_metric = 0.0\n",
    "    patience_counter = 0\n",
    "\n",
    "    for epoch in range(MAX_EPOCHS):\n",
    "        model.train()\n",
    "        total_loss = 0\n",
    "        with tqdm(train_loader, desc=f\"Epoch {epoch+1}/{MAX_EPOCHS}\") as pbar:\n",
    "            for x_batch, y_batch in pbar:\n",
    "                users = x_batch[:, 0].to(device)\n",
    "                items = x_batch[:, 1].to(device)\n",
    "                y_batch = y_batch.to(device)\n",
    "\n",
    "                if users.max().item() >= num_users or items.max().item() >= num_items:\n",
    "                    raise ValueError(\"‚ö†Ô∏è –ò–Ω–¥–µ–∫—Å—ã –≤—ã—Ö–æ–¥—è—Ç –∑–∞ –¥–∏–∞–ø–∞–∑–æ–Ω!\")\n",
    "\n",
    "                with autocast(device_type=device.type):\n",
    "                    logits = model(users, items)\n",
    "                    loss = criterion(logits, y_batch)\n",
    "\n",
    "                scaler.scale(loss).backward()\n",
    "                scaler.step(optimizer)\n",
    "                scaler.update()\n",
    "                optimizer.zero_grad(set_to_none=True)\n",
    "\n",
    "                total_loss += loss.item()\n",
    "                pbar.set_postfix(loss=loss.item())\n",
    "\n",
    "        # --- –í–∞–ª–∏–¥–∞—Ü–∏—è ---\n",
    "        model.eval()\n",
    "        all_preds = []\n",
    "        all_true = []\n",
    "\n",
    "        with torch.no_grad():\n",
    "            for x_batch, y_batch in val_loader:\n",
    "                users = x_batch[:, 0].to(device)\n",
    "                items = x_batch[:, 1].to(device)\n",
    "                scores = model(users, items)\n",
    "                probs = torch.sigmoid(scores) > 0.5\n",
    "                all_preds.extend(probs.cpu())\n",
    "                all_true.extend(y_batch.cpu())\n",
    "\n",
    "        val_recall = recall_score(all_true, all_preds, average='binary')\n",
    "        print(f\"Epoch {epoch+1} | Loss: {total_loss / len(train_loader):.4f} | Val Recall@40: {val_recall:.4f}\")\n",
    "\n",
    "        # Early Stopping\n",
    "        if val_recall > best_metric:\n",
    "            best_metric = val_recall\n",
    "            torch.save(model.state_dict(), BEST_MODEL_PATH)\n",
    "            patience_counter = 0\n",
    "        else:\n",
    "            patience_counter += 1\n",
    "\n",
    "        if patience_counter >= PATIENCE:\n",
    "            print(\"Early stopping triggered\")\n",
    "            break\n",
    "\n",
    "    return model, device\n",
    "\n",
    "# ==============================\n",
    "# üëÄ build_seen_nodes ‚Äî –ø—Ä–æ—Å–º–æ—Ç—Ä–µ–Ω–Ω—ã–µ —Ç–æ–≤–∞—Ä—ã\n",
    "# ==============================\n",
    "def build_seen_nodes(clickstream):\n",
    "    seen = defaultdict(set)\n",
    "    for _, row in clickstream.iterrows():\n",
    "        cookie = row['cookie']\n",
    "        node = row['node']\n",
    "        if pd.notna(cookie) and pd.notna(node):\n",
    "            seen[cookie].add(str(node))  # –ü—Ä–∏–≤–µ–¥–µ–Ω–∏–µ –∫ —Å—Ç—Ä–æ–∫–µ\n",
    "    return seen\n",
    "\n",
    "# ==============================\n",
    "# üß± –£—Å–∫–æ—Ä–µ–Ω–Ω–∞—è –≥–µ–Ω–µ—Ä–∞—Ü–∏—è item —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤\n",
    "# ==============================\n",
    "def compute_item_embeddings(model, device, all_nodes, le_item, num_items):\n",
    "    print(\"–ì–µ–Ω–µ—Ä–∞—Ü–∏—è item —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –ø–æ –±–∞—Ç—á–∞–º...\")\n",
    "    valid_node_indices = []\n",
    "\n",
    "    for node in all_nodes:\n",
    "        if str(node) in le_item.classes_.astype(str):\n",
    "            idx = le_item.transform([str(node)])[0]\n",
    "            valid_node_indices.append(idx)\n",
    "        else:\n",
    "            valid_node_indices.append(num_items + 1)  # –≤–Ω–µ –¥–∏–∞–ø–∞–∑–æ–Ω–∞ ‚Üí –∏—Å–∫–ª—é—á–µ–Ω–æ\n",
    "\n",
    "    item_ids = torch.tensor(valid_node_indices, dtype=torch.long).to(device)\n",
    "    embeddings_list = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for i in range(0, len(item_ids), BATCH_SIZE_ITEMS):\n",
    "            batch = item_ids[i:i+BATCH_SIZE_ITEMS]\n",
    "            emb = model.get_item_vector(batch).cpu().numpy()\n",
    "            embeddings_list.append(emb)\n",
    "            print(f\"–û–±—Ä–∞–±–æ—Ç–∞–Ω–æ {i + BATCH_SIZE_ITEMS} / {len(item_ids)} item'–æ–≤\")\n",
    "\n",
    "    item_embeddings = np.vstack(embeddings_list)\n",
    "    with open(ITEM_EMBEDDINGS_FILE, 'wb') as f:\n",
    "        pickle.dump(item_embeddings, f)\n",
    "    print(\"Item —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã\")\n",
    "    return item_embeddings, [all_nodes[i] for i in valid_node_indices]\n",
    "\n",
    "# ==============================\n",
    "# üß∞ –£—Å–∫–æ—Ä–µ–Ω–Ω—ã–π –∏–Ω—Ñ–µ—Ä–µ–Ω—Å —á–µ—Ä–µ–∑ FAISS\n",
    "# ==============================\n",
    "def recommend_for_users_resumable(model, device, test_users, all_nodes, le_user, seen_dict, top_k=40):\n",
    "    model.eval()\n",
    "    \n",
    "    # 1Ô∏è‚É£ –ü–æ–ª—É—á–∞–µ–º item —ç–º–±–µ–¥–¥–∏–Ω–≥–∏\n",
    "    if os.path.exists(ITEM_EMBEDDINGS_FILE):\n",
    "        print(\"–ó–∞–≥—Ä—É–∑–∫–∞ item —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –∏–∑ –∫—ç—à–∞...\")\n",
    "        with open(ITEM_EMBEDDINGS_FILE, 'rb') as f:\n",
    "            item_embeddings = pickle.load(f)\n",
    "    else:\n",
    "        item_embeddings = compute_item_embeddings(model, device, all_nodes, le_item, num_items)\n",
    "\n",
    "    # 2Ô∏è‚É£ –°–æ–∑–¥–∞—ë–º FAISS index\n",
    "    d = item_embeddings.shape[1]  # —Ä–∞–∑–º–µ—Ä–Ω–æ—Å—Ç—å —ç–º–±–µ–¥–¥–∏–Ω–≥–∞\n",
    "    index = faiss.IndexFlatIP(d)  # inner product\n",
    "    index.add(item_embeddings.astype(np.float32))\n",
    "\n",
    "    # 3Ô∏è‚É£ –ü–æ–¥–≥–æ—Ç–æ–≤–∫–∞ —Ç–µ—Å—Ç–æ–≤—ã—Ö –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\n",
    "    valid_cookies = []\n",
    "    encoded_ids = []\n",
    "    for _, row in test_users.iterrows():\n",
    "        cookie = row['cookie']\n",
    "        if cookie in le_user.classes_:\n",
    "            try:\n",
    "                encoded_id = int(le_user.transform([cookie])[0])\n",
    "                encoded_ids.append(encoded_id)\n",
    "                valid_cookies.append(str(cookie))\n",
    "            except Exception as e:\n",
    "                print(f\"–û—à–∏–±–∫–∞ –∫–æ–¥–∏—Ä–æ–≤–∞–Ω–∏—è cookie {cookie}: {e}\")\n",
    "\n",
    "    processed_cookies = set()\n",
    "    predictions = []\n",
    "\n",
    "    if os.path.exists(PREDICTIONS_FILE):\n",
    "        df_prev = pd.read_csv(PREDICTIONS_FILE)\n",
    "        predictions = df_prev.values.tolist()\n",
    "        print(f\"–ó–∞–≥—Ä—É–∂–µ–Ω–æ {len(df_prev)} –∑–∞–ø–∏—Å–µ–π –∏–∑ –ø—Ä–µ–¥—ã–¥—É—â–µ–≥–æ –∑–∞–ø—É—Å–∫–∞.\")\n",
    "        processed_cookies.update(df_prev['cookie'].astype(str).unique())\n",
    "\n",
    "    remaining = [(c, e) for c, e in zip(valid_cookies, encoded_ids) if c not in processed_cookies]\n",
    "    print(f\"–û—Å—Ç–∞–ª–æ—Å—å –æ–±—Ä–∞–±–æ—Ç–∞—Ç—å: {len(remaining)} –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª–µ–π\")\n",
    "\n",
    "    start = time.time()\n",
    "    for i in range(0, len(remaining), BATCH_SIZE_INFERENCE):\n",
    "        batch = remaining[i:i+BATCH_SIZE_INFERENCE]\n",
    "        cookies_batch, encoded_batch = zip(*batch)\n",
    "        user_tensor = torch.LongTensor(encoded_batch).to(device)\n",
    "\n",
    "        with torch.no_grad():\n",
    "            user_vectors = model.get_user_vector(user_tensor).cpu().numpy()\n",
    "\n",
    "        # 4Ô∏è‚É£ –ü–æ–∏—Å–∫ —á–µ—Ä–µ–∑ FAISS\n",
    "        D, I = index.search(user_vectors.astype(np.float32), top_k=top_k)\n",
    "\n",
    "        for idx, cookie in enumerate(cookies_batch):\n",
    "            ranked = [\n",
    "                (all_nodes[I[idx][j]], float(D[idx][j]))\n",
    "                for j in range(top_k)\n",
    "                if str(all_nodes[I[idx][j]]) not in seen_dict.get(cookie, set())\n",
    "            ]\n",
    "            for node, score in ranked[:top_k]:\n",
    "                predictions.append([str(node), str(cookie), score])\n",
    "            processed_cookies.add(cookie)\n",
    "\n",
    "        if (i // BATCH_SIZE_INFERENCE) % 10 == 0:\n",
    "            pd.DataFrame(predictions, columns=['node', 'cookie', 'score']).to_csv(PREDICTIONS_FILE, index=False)\n",
    "            elapsed = time.time() - start\n",
    "            print(f\"[{i + len(batch):>6}/{len(valid_cookies)}] —Å–æ—Ö—Ä–∞–Ω–µ–Ω–æ... [–í—Ä–µ–º—è: {elapsed:.2f} —Å–µ–∫.]\")\n",
    "\n",
    "    print(\"–ò–Ω—Ñ–µ—Ä–µ–Ω—Å –∑–∞–≤–µ—Ä—à—ë–Ω.\")\n",
    "    return pd.DataFrame(predictions, columns=['node', 'cookie', 'score'])\n",
    "\n",
    "# ==============================\n",
    "# üíæ –°–æ—Ö—Ä–∞–Ω–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞\n",
    "# ==============================\n",
    "def save_submission(df, path=\"submission.csv\"):\n",
    "    df[['node', 'cookie']].to_csv(path, index=False)\n",
    "    print(f\"–†–µ–∑—É–ª—å—Ç–∞—Ç —Å–æ—Ö—Ä–∞–Ω—ë–Ω –≤ {path}\")\n",
    "\n",
    "# ==============================\n",
    "# üöÄ MAIN –ø–∞–π–ø–ª–∞–π–Ω\n",
    "# ==============================\n",
    "if __name__ == \"__main__\":\n",
    "    try:\n",
    "        t_start = time.time()\n",
    "\n",
    "        # –®–∞–≥ 1: –ó–∞–≥—Ä—É–∑–∫–∞ –∏–ª–∏ –≤–æ—Å—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω–∏–µ –∏–∑ –∫—ç—à–∞\n",
    "        if os.path.exists(SAMPLED_DATA_CACHE):\n",
    "            print(\"–ó–∞–≥—Ä—É–∑–∫–∞ –ø–æ–¥–≥–æ—Ç–æ–≤–ª–µ–Ω–Ω—ã—Ö –¥–∞–Ω–Ω—ã—Ö –∏–∑ –∫—ç—à–∞...\")\n",
    "            with open(SAMPLED_DATA_CACHE, 'rb') as f:\n",
    "                grouped = pickle.load(f)\n",
    "            with open(ENCODER_CACHE, 'rb') as f:\n",
    "                le_user, le_item = pickle.load(f)\n",
    "            num_users = le_user.classes_.shape[0]\n",
    "            num_items = le_item.classes_.shape[0]\n",
    "        else:\n",
    "            print(\"–ù–∞—á–∞–ª–æ –ø–æ–ª–Ω–æ–π –ø–æ–¥–≥–æ—Ç–æ–≤–∫–∏ –¥–∞–Ω–Ω—ã—Ö...\")\n",
    "            data = load_data()\n",
    "            grouped = prepare_pairs(data['clickstream'], data['events'])\n",
    "            if IS_DEBUG:\n",
    "                grouped = sample_data(grouped)\n",
    "            grouped, num_users, num_items, le_user, le_item = encode_user_item(grouped)\n",
    "            with open(SAMPLED_DATA_CACHE, 'wb') as f:\n",
    "                pickle.dump(grouped, f)\n",
    "            with open(ENCODER_CACHE, 'wb') as f:\n",
    "                pickle.dump((le_user, le_item), f)\n",
    "            print(\"–î–∞–Ω–Ω—ã–µ –∑–∞–∫–æ–¥–∏—Ä–æ–≤–∞–Ω—ã –∏ —Å–æ—Ö—Ä–∞–Ω–µ–Ω—ã –≤ –∫—ç—à\")\n",
    "\n",
    "        print(f\"num_users: {num_users}, num_items: {num_items}\")\n",
    "        print(f\"–ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π user_id: {grouped['user_id'].max()}, –ú–∞–∫—Å–∏–º–∞–ª—å–Ω—ã–π item_id: {grouped['item_id'].max()}\")\n",
    "\n",
    "        # –®–∞–≥ 2: –û–±—É—á–µ–Ω–∏–µ –º–æ–¥–µ–ª–∏\n",
    "        model, device = train_model_with_validation(grouped, num_users, num_items)\n",
    "\n",
    "        # –®–∞–≥ 3: –ò–Ω—Ñ–µ—Ä–µ–Ω—Å —Å FAISS\n",
    "        data = load_data()\n",
    "        seen_dict = build_seen_nodes(data['clickstream'])\n",
    "\n",
    "        submission = recommend_for_users_resumable(\n",
    "            model=model,\n",
    "            device=device,\n",
    "            test_users=data['test_users'],\n",
    "            all_nodes=data['cat_features']['node'].unique().astype(str),\n",
    "            le_user=le_user,\n",
    "            seen_dict=seen_dict\n",
    "        )\n",
    "        save_submission(submission)\n",
    "\n",
    "        t_end = time.time()\n",
    "        print(f\"–û–±—â–µ–µ –≤—Ä–µ–º—è –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è: {(t_end - t_start)/60:.2f} –º–∏–Ω\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"‚ö†Ô∏è –ö—Ä–∏—Ç–∏—á–µ—Å–∫–∞—è –æ—à–∏–±–∫–∞ –≤ main: {e}\")\n",
    "        raise"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "279ae7f7-ed20-42a0-82c4-7f58a4570a65",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d11e57cd-57fc-403c-bf3d-3596a1151f33",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
